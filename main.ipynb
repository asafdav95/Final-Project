{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18967,"status":"ok","timestamp":1686720799964,"user":{"displayName":"asaf davidovitch","userId":"12569593543357363445"},"user_tz":-180},"id":"qgBxomO5U70J","outputId":"5799fc06-b8d2-433a-fc4e-c67c5c7643a2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tensorflow==2.8.0 in /usr/local/lib/python3.10/dist-packages (2.8.0)\n","Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.6.3)\n","Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (23.3.3)\n","Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (0.4.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (3.8.0)\n","Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.1.2)\n","Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (16.0.0)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.22.4)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (3.3.0)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (2.3.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (4.5.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.14.1)\n","Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (2.8.0)\n","Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (2.8.0.dev2021122109)\n","Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (2.8.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (0.32.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.54.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.8.0) (0.40.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.17.3)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.4.6)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.4.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.27.1)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.6.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.8.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.3.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (5.3.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.3.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.4)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.1.2)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.2.2)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: scikit-learn-extra in /usr/local/lib/python3.10/dist-packages (0.3.0)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-extra) (1.22.4)\n","Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-extra) (1.10.1)\n","Requirement already satisfied: scikit-learn>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-extra) (1.2.2)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (3.1.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n","Requirement already satisfied: install in /usr/local/lib/python3.10/dist-packages (1.3.5)\n","Requirement already satisfied: dtaidistance in /usr/local/lib/python3.10/dist-packages (2.3.10)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from dtaidistance) (1.22.4)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pydot in /usr/local/lib/python3.10/dist-packages (1.4.2)\n","Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.10/dist-packages (from pydot) (3.0.9)\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["!pip install tensorflow==2.8.0\n","!pip install scikit-learn-extra\n","!pip install pip install dtaidistance\n","!apt-get -qq install -y graphviz && pip install pydot\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import sys\n","from dtaidistance import dtw\n","from dtaidistance import dtw_visualisation as dtwvis\n","\n","\n","import nltk\n","nltk.download('punkt')\n","\n","import csv\n","from sklearn.manifold import TSNE\n","import matplotlib.pyplot as plt\n","\n","import numpy as np\n","import random\n","#from fastdtw import fastdtw\n","from sklearn_extra.cluster import KMedoids\n","from scipy.spatial.distance import euclidean\n","\n","import os\n","from sklearn.manifold import TSNE\n","\n","import keras\n","from keras import Sequential\n","import tensorflow as tf\n","from keras.layers import Dense, Bidirectional, Dropout\n","from keras.layers import LSTM\n","#from keras.layers.recurrent import LSTM\n","from keras.layers.convolutional import MaxPooling1D, Conv1D\n","from keras.callbacks import EarlyStopping\n","from keras.layers import Layer\n","from keras import backend as K\n","import gc\n","\n","\n","import string\n","from nltk.corpus import stopwords\n","import gc\n","nltk.download(\"stopwords\")\n","\n","from keras.layers import Embedding\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","import warnings\n","warnings.filterwarnings(action='ignore')\n","from gensim.models import Word2Vec\n","import gensim.downloader as api\n","import tensorflow_hub as hub\n","import pickle\n","import glob\n","import math\n","import statistics\n","import seaborn as sns\n","import pandas as pd\n","from sklearn.ensemble import IsolationForest\n","from scipy.stats import rankdata\n","import gensim\n","import gensim.downloader as api\n","#t_model = api.load('word2vec-google-news-300')\n","#word_vectors = t_model.vectors\n","\n","#!pip install ann_visualizer\n","#from ann_visualizer.visualize import ann_viz;\n","import pydot\n","from keras.utils.vis_utils import plot_model\n","\n","import pdb\n","\n","colab_path = \"/content/drive/MyDrive/Lee_sh/LEE/final_project2/\" #path for the folder in the drive\n","colab_path0 = \"/content/drive/MyDrive/Lee_sh/LEE/res700/\" #path for the folder in the drive\n","py_file_location = \"/content/drive/MyDrive/Lee_sh/LEE/final_project2\"\n","sys.path.append(os.path.abspath(py_file_location))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tW5rwlstVE-J"},"outputs":[],"source":["def classify_text(params_obj):\n","    global iteration_num, sum_dicForest,sum_dic_summ,k_medoids,num_lstm,k_medoidsL,dist_mat2,dist_mat,dict_test2, t_model\n","    try:\n","        emb_dim = int(params_obj['dimensional'])\n","        chunk_size = int(params_obj['chunk'])\n","        #print(chunk_size)\n","        batch_factor = 12\n","        data_batch_size = chunk_size // batch_factor\n","        print(chunk_size,data_batch_size)\n","        iter_idx = iteration_num          # iteration number\n","        kernel_size = [3, 6, 12]\n","        nb_filter = 500*2\n","        pool_size = 1\n","        dense_outputs = 512*4\n","        cat_output = 2\n","        learning_rate = 0.001\n","        momentum = 0.9\n","        decay = 1\n","        nb_epoch = 5\n","        num_lstm=500\n","        lstm_out = int(num_lstm * 2)\n","        test_size = 0.25\n","        pr = 0.00005 / 2\n","        forest_num=100\n","        k = 2\n","        pr0 = 90\n","\n","        V_acc = []\n","        sum_dicForest = sum_dicForest\n","        general_dic = {}\n","\n","        # get data base\n","        imposter_a_list = data_base_dict[params_obj[\"imposter_a\"]]\n","        imposter_b_list = data_base_dict[params_obj[\"imposter_b\"]]\n","        author_data_list = data_base_dict[\"shakespeare\"]\n","        #print(author_data_list)\n","        files = getBooksNameList(\"shakespeare\")\n","        files = [file[:-4] for file in files]\n","\n","        #print(files)\n","        #print(params_obj['file_path'])\n","        #print(params_obj[\"imposter_a\"])\n","        # load text from file_path\n","        path0='/content/drive/MyDrive/Lee_sh/LEE/final_project2/data_base/shakespeare/text to classify.txt'\n","       # with open(params_obj['file_path'], 'r', encoding=\"utf-8\") as file:\n","        with open(path0, 'r', encoding=\"utf-8\") as file:\n","            text_to_classify = file.read()\n","            text_name = os.path.basename(file.name)\n","            lines = file.readlines()\n","            lines = [line.rstrip() for line in lines]\n","        #author_data_list.append(text_to_classify)\n","        #files.append(text_name[:-4])\n","        #print(np.asarray(files).shape)\n","\n","        # -------- PREPROCESSING --------\n","        print(\"---------------------------\")\n","        print(\"Start Preprocessing\")\n","        imposter_a_list = preprocessing_stage(imposter_a_list, chunk_size, emb_dim)\n","        imposter_b_list = preprocessing_stage(imposter_b_list, chunk_size, emb_dim)\n","        author_data_list = preprocessing_stage(author_data_list, chunk_size, emb_dim)\n","        print(\"Finish Preprocessing\")\n","        print(\"---------------------------\")\n","        # -------------------------------\n","\n","        # ----------------WORD EMBEDDING--------------------------------------------------------------\n","        # check if word2vec or elmo\n","        print(\"Start Word Embedding\")\n","        model, total_examples = word2vec_algorithm(imposter_a_list, imposter_b_list, author_data_list, emb_dim, params_obj['embedding_algorithm'])\n","        model = fine_tune_word2vec_model(model, imposter_a_list, imposter_b_list, author_data_list, emb_dim, total_examples, model_type=\"Word2vec google\")\n","        print(\"Finish Word Embedding\")\n","        print(\"---------------------------\")\n","\n","        test0 = []\n","        for data in author_data_list:\n","            asdf = emm0W(data, model, data_batch_size)\n","            test0.append(asdf)\n","\n","        imp_1 = emm0W_to_collection(imposter_a_list, model, data_batch_size)\n","        imp_2 = emm0W_to_collection(imposter_b_list, model, data_batch_size)\n","\n","        X, Y, imp1 = create_XY(imp_1, imp_2)\n","        # --------------------------------------------------------------------------------------------\n","\n","        # ----------------RCNNA-----------------------------------------------------------------------\n","        print(\"Start RCNNA\")\n","        rcnna_model, history = rcnna(X, Y, emb_dim, data_batch_size, kernel_size, nb_filter,\n","            pool_size, dense_outputs, cat_output, learning_rate, momentum, decay,\n","            nb_epoch, lstm_out, test_size, 0, mod_name='model_eng', DropoutP=0.50)\n","        #keras2ascii(rcnna_model)\n","        #rcnna_model.save('model.h5')\n","\n","        #netron.start('model.h5')\n","\n","        plot_model(rcnna_model, to_file='model.png')\n","        rcnna_model.save(colab_path0+'best_model'+str(iteration_num)+'.h5')\n","        print(\"history['accuracy'] = \", history.history['accuracy'][-1])\n","        print(\"history['val_accuracy'] = \", history.history['val_accuracy'][-1])\n","        print(\"Finish RCNNA\")\n","        print(\"---------------------------\")\n","\n","        V_acc.append(history.history['val_accuracy'][-1])\n","        #np.save(colab_path0+\"V_acc \"+str(iteration_num)+\".npy\", V_acc)\n","\n","        #Print figures\n","        plt.plot(history.history['accuracy'])\n","        plt.plot(history.history['val_accuracy'])\n","        plt.title('model accuracy')\n","        plt.ylabel('accuracy')\n","        plt.xlabel('epoch')\n","        plt.legend(['train', 'test'], loc='upper left')\n","        plt.show()\n","        #plt.savefig(colab_path0+\"Acc \"+str(iteration_num)+\".png\")\n","\n","        # summarize history for loss\n","        plt.plot(history.history['loss'])\n","        plt.plot(history.history['val_loss'])\n","        plt.title('model loss')\n","        plt.ylabel('loss')\n","        plt.xlabel('epoch')\n","        plt.legend(['train', 'test'], loc='upper left')\n","        plt.show()\n","        #plt.savefig(colab_path0+\"Loss \"+str(iteration_num)+\".png\")\n","        #plt.show()\n","        # --------------------------------------------------------------------------------------------\n","\n","        print(\"---------------------------\")\n","        print(\"Distance Matrix\")\n","\n","        dict_test = {}\n","        for i, file in enumerate(files):\n","            if file!='text_to_classify' and i <= 49:\n","              dict_test[file] = np.asarray(rcnna_model.predict(np.asarray(test0[i])))[:, 0]\n","        #print(files)\n","        #print(i)\n","        del X, Y\n","        if 'Names' not in sum_dicForest:\n","            sum_dicForest['Names'] = files\n","            sum_labels['Names'] = files\n","            sum_labelsS['Names'] = files\n","            sum_dic_summ['Names'] = files\n","\n","        dict_test2 = {}\n","\n","        for key in dict_test:\n","            data = dict_test[key]\n","            list0 = [sum(g) / batch_factor for g in zip(*[iter(data)] * batch_factor)]\n","            dict_test2[key] = list0\n","            general_dic[key] = []\n","            general_dic[key].append(list0)\n","        #print(dict_test2)\n","        m = len(dict_test2)\n","\n","        dist_mat = np.zeros([m, m])\n","        dist_matS=dist_mat\n","\n","        for key in list(dict_test2):\n","            if len(dict_test2[key]) < 2:\n","                try:\n","                  dict_test2.pop(key)\n","                  sum_dicForest.pop(key)\n","                  sum_dic_summ.pop(key)\n","                except:\n","                  print()\n","\n","        for i, key1 in enumerate(dict_test2.keys()):\n","            for j, key2 in enumerate(dict_test2.keys()):\n","                #distance0, path = fastdtw(dict_test2[key1], dict_test2[key2],dist=euclidean)\n","                distance0 = dtw.distance(dict_test2[key1], dict_test2[key2])\n","                dist_mat[i, j] = distance0\n","        #dist_mat=(np.transpose(dist_mat) +dist_mat)*0.5\n","        #print(dist_mat)\n","\n","        np.save(colab_path0+\"Signal\"+str(iteration_num)+str(num_lstm)+\".npy\", dict_test2)\n","        np.save(colab_path0+\"dict_mat00 \"+str(iteration_num)+\".npy\", dist_mat)\n","        #np.asarray(dist_mat).tofile(colab_path+\"dist_mat\"+str(iteration_num)+\".csv\",sep=',',format='%10.5f')\n","        #dist_mat0=np.concatenate((dist_mat0, dist_mat), axis=0)\n","        dist_matS=dist_mat\n","        print(\"Finish Distance Matrix\")\n","        print(\"---------------------------\")\n","\n","        print(\"---------------------------\")\n","        print(\"Start presentation\")\n","        summa, score, y, rang = presentation(dist_mat, pr, forest_num, files, k, pr0, 'DWT', lines)\n","        sum_dicForest[iter_idx] = score\n","        sum_dic_summ[iter_idx]=summa\n","        print(\"Finish presentation\")\n","        print(\"---------------------------\")\n","\n","        # create k-medoids model\n","        print(\"---------------------------\")\n","        print(\"Start K-Medoids\")\n","\n","        kmedoids = KMedoids(n_clusters=2, random_state=0).fit(dist_mat)\n","        # update the global params for the next iteration\n","        sum_dicForest = sum_dicForest\n","        k_medoidsL = kmedoids.labels_\n","        #k_medoids_data = dist_mat0\n","        #kmedoidsS = KMedoids(n_clusters=2, random_state=0).fit(dist_matS)\n","        print(\"Finish K-Medoids\")\n","        print(\"---------------------------\")\n","        print(\"Move to result page\")\n","    except Exception as e:\n","        print(\"ERROR in classify_text: \")\n","        print(e)\n","        raise Exception(e)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F47b4udtVJ-f"},"outputs":[],"source":["params_obj = {\n","            'file_path': \"\",\n","            'imposter_a': \"\",\n","            'imposter_b': \"\",\n","            'embedding_algorithm': \"\",\n","            'dimensional': \"\",\n","            'chunk': \"\"\n","        }\n","params_obj_list = []\n","iteration_num = 5\n","\n","screen_dict = {}\n","\n","data_base_dict = {}\n","sum_dic_summ = {}\n","sum_dicForest = {}\n","sum_labels={}\n","sum_labelsS={}\n","#k_medoids = []\n","\n","k_medoids_data = []\n","\n","#------------------------------------------------------------------------------------\n","#---------------------MAIN-----------------------------------------------------------\n","#------------------------------------------------------------------------------------\n","def main():\n","    global iteration_num, sum_dicForest,sum_dic_summ,k_medoids,num_lstm,k_medoidsL,dist_mat2,dist_mat,dict_test2, t_model\n","    get_shakespeare_data()    # load shakespeare data\n","    imposters_names = get_imposters_name_list()\n","    #np.save(colab_path0+\"names\"+\".npy\", imposters_names)\n","    imposters_names0=imposters_names\n","    slu_list0=[]\n","    #print(len(imposters_names))\n","    for i in range(len(imposters_names)):\n","     for j in range(i+1,len(imposters_names)):\n","      slu_list0.append((i,j))\n","\n","\n","    i0=slu_list0[iteration_num-1][0]\n","    j0=slu_list0[iteration_num-1][1]\n","    #print(slu_list0[iteration_num-1])\n","    gr=(len(imposters_names)* (len(imposters_names)-1))//2\n","    #for i in range(i0,len(imposters_names)):\n","        #for j in range(j0, len(imposters_names)):\n","    while(iteration_num<(gr+1)):\n","            i=slu_list0[iteration_num-1][0]\n","            j=slu_list0[iteration_num-1][1]\n","            print(slu_list0[iteration_num-1])\n","\n","            print(i,j)\n","            # load imposters data\n","            get_imposter_by_name(imposters_names[i])\n","            get_imposter_by_name(imposters_names[j])\n","            # params\n","            p = {\n","                'file_path': colab_path + \"data_base/text to classify.txt\",  # text to classify\n","                'imposter_a': imposters_names[i],\n","                'imposter_b': imposters_names[j],\n","                #'embedding_algorithm': \"Word2vec old english\",\n","                'embedding_algorithm': \"Word2vec google\",\n","                'dimensional': \"300\",\n","                'chunk': \"600\"\n","            }\n","\n","            print('Iteration= \\n',iteration_num)\n","            print(imposters_names0[i],'  ',imposters_names0[j],'\\n')\n","\n","            # start classify\n","            classify_text(p)\n","\n","\n","    # save report\n","\n","            sum_dicForest = sum_dicForest\n","            sum_dic_summ=sum_dic_summ\n","            #sum_dicForest['Labels'] = k_medoids.labels_\n","            sum_labels[str(iteration_num)] = k_medoidsL\n","            #sum_labelsS[str(iteration_num)]=k_medoidsL\n","            columns = sum_dicForest.keys()\n","            header = columns\n","            with open(colab_path0+\"res_report\"+str(iteration_num)+str( num_lstm)+\".csv\", 'w', newline='') as csvfile:\n","             no_rows = len(sum_dicForest[list(columns)[0]])\n","             csvwriter = csv.writer(csvfile, delimiter=',')\n","             csvwriter.writerow(header)\n","             for row in range(no_rows-1):\n","              csvwriter.writerow([sum_dicForest[key][row] for key in header])\n","\n","            with open(colab_path0+\"dist_mat00\"+str(iteration_num)+str( num_lstm)+\".csv\", 'w', newline='') as csvfile:\n","              np.savetxt(csvfile, dist_mat, delimiter=\",\")\n","\n","\n","\n","            #np.savetxt(colab_path0+\"Signal\"+str(iteration_num)+str(num_lstm) +\".csv\", dictlist, delimiter=\",\")\n","            #np.save(colab_path0+\"Signal\"+str(iteration_num)+str(num_lstm)+\".npy\", dictlist, allow_pickle=True)\n","           # np.save(colab_path0+\"Signal\"+str(iteration_num)+str(num_lstm)+\".npy\",dictlist)\n","\n","\n","\n","            with open(colab_path0+\"res_report_Dist\"+str(iteration_num)+str(num_lstm)+\".csv\", 'w', newline='') as csvfile:\n","             no_rows = len(sum_labels[list(columns)[0]])\n","             csvwriter = csv.writer(csvfile, delimiter=',')\n","             csvwriter.writerow(sum_labels.keys())\n","             for row in range(no_rows-1):\n","              csvwriter.writerow([sum_labels[key][row] for key in sum_labels.keys()])\n","            #csvwriter.close()\n","            #sum_dicForest.pop('Labels')\n","\n","\n","           # np.save(colab_path+\"dist_mat2\"+str(iteration_num)+\".npy\", dist_mat2)\n","            np.save(colab_path0+\"dist_mat_\"+str(iteration_num)+\".npy\", dist_mat)\n","\n","            pd.DataFrame(dist_mat).to_csv(colab_path0+\"dist_mat_\"+str(iteration_num)+\".csv\")\n","\n","            iteration_num += 1\n","\n","\n","\n","\n","\n","#---------------------END MAIN-------------------------------------------------------\n","#------------------------------------------------------------------------------------\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GMPkP8UjVLg6"},"outputs":[],"source":["def rcnna(X, Y, emb_dim, data_batch_size, kernel_size, nb_filter, pool_size, dense_outputs=256,\n","          cat_output=10, learning_rate=0.001, momentum=0.9, decay=0, nb_epoch=12, lstm_out=150,\n","          test_size=0.33, usePreL=0, mod_name='model_eng', DropoutP=0.25):\n","    # global model, word_model\n","\n","    print('Data:')\n","    print(X.shape)\n","    print(Y.shape)\n","\n","    X_train, Y_train = X, Y\n","    # Creating a model\n","    model = Sequential()\n","    for i in range(len(kernel_size)):\n","        model.add(Conv1D(filters=nb_filter, kernel_size=kernel_size[i], padding='valid', activation='relu',\n","                         input_shape=(data_batch_size, emb_dim)))\n","        model.add(MaxPooling1D(pool_size=pool_size))\n","    model.add(Bidirectional(LSTM(units=lstm_out, return_sequences=True), merge_mode='concat',\n","                            input_shape=(X_train.shape[1], X_train.shape[2])))\n","    model.add(Bidirectional(LSTM(units=lstm_out, go_backwards=True)))\n","\n","    #model.add(Attention(return_sequences=True))\n","\n","    #model.add(BahdanauAttention(cat_output))\n","    model.add(Dense(dense_outputs, activation='sigmoid'))\n","    model.add(Dropout(DropoutP))\n","    model.add(Dense(cat_output, activation='sigmoid'))\n","\n","    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n","    #ann_viz(model, view=True, title=\"test\", filename=\"visualized\")\n","\n","    print(model.summary())\n","    print('Fit model...')\n","\n","    Y_tmp = np.zeros([Y_train.shape[0], 2])\n","    Y_tmp[:, 0] = 2 - Y_train\n","    Y_tmp[:, 1] = Y_train - 1\n","    Y_train = Y_tmp\n","\n","    history = model.fit(X_train, Y_train, validation_split=test_size, epochs=nb_epoch, verbose=1)\n","                      #  callbacks=[EarlyStopping(monitor='val_accuracy', patience=0, restore_best_weights=True)])\n","\n","    del X, Y\n","\n","    gc.collect()\n","\n","    return model, history\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EpJcctIoVTu5"},"outputs":[],"source":["def create_XY(imp_1, imp_2):\n","    imp1 = [imp_1, imp_2]\n","    del imp_2\n","    l0 = [len(imp1[0]), len(imp1[1])]\n","    i1 = l0.index(max(l0))\n","    i2 = l0.index(min(l0))\n","    imp0 = []\n","\n","    for kk in range(l0[i1] // l0[i2]):\n","        imp0 = imp0 + imp1[i2]\n","\n","    imp1[i2] = imp0 + random.sample(imp1[i2], l0[i1] - len(imp0))\n","    imp1[0]=imp1[0]+imp1[0]+imp1[0]+imp1[0]\n","    #+imp1[0]\n","    imp1[1]=imp1[1]+imp1[1]+imp1[1]+imp1[1]\n","    #+imp1[1]\n","\n","    #imp1[0]=imp1[0]+imp1[0]\n","    #imp1[1]=imp1[1]+imp1[1]\n","\n","    Y = [1] * len(imp1[0]) + [2] * len(imp1[1])\n","    X = [y for x in [imp1[0], imp1[1]] for y in x]\n","\n","    X = np.asarray(X)\n","    Y = np.asarray(Y)\n","    return X, Y, imp1\n","\n","def emm0W_to_collection(collection, model, data_batch_size):\n","    res_list = []\n","    try:\n","        for data in collection:\n","            asdf = emm0W(data, model, data_batch_size)\n","            for a in asdf:\n","                res_list.append(a)\n","    except Exception as e:\n","        res_list = []\n","        print(e)\n","    finally:\n","        return res_list\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mrBqkpJFVVpf"},"outputs":[],"source":["class BahdanauAttention(Layer):\n","    def __init__(self, units):\n","        super(BahdanauAttention, self).__init__()\n","        self.W1 = tf.keras.layers.Dense(units)\n","        self.W2 = tf.keras.layers.Dense(units)\n","        self.V = tf.keras.layers.Dense(1)\n","\n","class Attention(Layer):\n","    def __init__(self, return_sequences=True):\n","        self.return_sequences = return_sequences\n","        super(Attention, self).__init__()\n","\n","    def build(self, input_shape):\n","        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1), initializer=\"normal\")\n","        self.b = self.add_weight(name=\"att_bias\", shape=(1,), initializer=\"zeros\")\n","        super(Attention, self).build(input_shape)\n","\n","    def call(self, x):\n","        e = K.tanh(K.dot(x, self.W) + self.b)\n","        a = K.softmax(e, axis=1)\n","        output = x * a\n","        if self.return_sequences:\n","            return output\n","        return K.sum(output, axis=1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TbAhURmiVXv3"},"outputs":[],"source":["def preprocessing_stage(collection, chunk_size, emb_dim=300):\n","    try:\n","        new_collection = []\n","        for text in collection:\n","            # split into words\n","            tokens = word_tokenize(text)\n","            # convert to lower case\n","            tokens = [w.lower() for w in tokens]\n","            # remove punctuation from each word\n","            table = str.maketrans('', '', string.punctuation)\n","            stripped = [w.translate(table) for w in tokens]\n","            # remove remaining tokens that are not alphabetic\n","            words = [word for word in stripped if word.isalpha()]\n","            # filter out stop words\n","            stop_words = set(stopwords.words('english'))\n","            words = [w for w in words if not w in stop_words]\n","\n","            new_collection.append(words)\n","        return new_collection\n","    except Exception as e:\n","        print(\"ERROR in preprocessing_stage: \" + e.args[0])\n","        return None\n","\n","def emm0W(impost_1, t_model, data_batch_size):\n","    impost_1_np_3D = []\n","    for token in impost_1:\n","        if token in t_model.wv:\n","            impost_1_np_3D.append(t_model.wv[token])\n","\n","    impost_1_np_3D = np.asarray(impost_1_np_3D)\n","    bloks = len(impost_1_np_3D)//data_batch_size\n","    X_1 = []\n","    for kk in range(bloks):\n","        X_1.append(impost_1_np_3D[kk*data_batch_size:(kk+1)*data_batch_size ,  :])\n","    X_1 = np.asarray(X_1)\n","    del impost_1_np_3D\n","    gc.collect()\n","    return X_1\n","\n","\n","def word2vec_algorithm(imposter_data1, imposter_data2, author_data, dimension, model_type=\"Word2vec google\"):\n","  try:\n","    global colab_path\n","\n","    model_2 = Word2Vec(vector_size=dimension, min_count=5)\n","    model_2.build_vocab(imposter_data1 + imposter_data2)\n","    total_examples = model_2.corpus_count\n","    if model_type == \"Word2vec google\":\n","      t_model = api.load('word2vec-google-news-300')\n","      word_vectors = t_model.vectors\n","      model = t_model\n","      model_2.build_vocab([list(model.key_to_index.keys())], update=True)\n","    else:\n","      with open(colab_path + \"/word-embedding-models/1800-vocab.pkl\", 'rb') as f:\n","          word = pickle.load(f)\n","      word_vectors = np.load(colab_path + \"/word-embedding-models/\" + \"1800-w.npy\")\n","      model_2.build_vocab(word, update=True)\n","\n","    model_2.intersect_word2vec_format = word_vectors\n","    model_2.train(imposter_data1 + imposter_data2 + author_data, total_examples=total_examples, epochs=20)\n","\n","    model_2.build_vocab([list(model_2.wv.key_to_index.keys())], update=True)\n","    model_2.train(imposter_data1 + imposter_data2 + author_data, total_examples=total_examples, epochs=10)\n","\n","    return model_2, total_examples\n","  except Exception as e:\n","    print(\"ERROR in word2vec_algorithm: \" + e.args[0])\n","    return None\n","\n","def fine_tune_word2vec_model(model, imposter_data1, imposter_data2, author_data, dimension, total_examples=2000, model_type=\"Word2vec google\"):\n","  try:\n","    for epoch in range(10):\n","      model.train(imposter_data1 + imposter_data2 + author_data, total_examples=total_examples, epochs=1)\n","\n","    return model\n","  except Exception as e:\n","    print(\"ERROR in fine tune word2vec model: \" + e.args[0])\n","    return None\n","\n","def get_shakespeare_data():\n","    global data_base_dict\n","    try:\n","      path = colab_path + \"data_base\"\n","      file_list = glob.glob(os.path.join(path, \"shakespeare\", \"*.txt\"))\n","      data_list = create_data_list(file_list)\n","      #print(data_list)\n","      #print(len(data_list))\n","      if 'text to classify' in set(data_list):\n","        data_list.remove('text to classify')\n","      data_base_dict['shakespeare'] = data_list\n","    except Exception as e:\n","      print(\"Error at get_shakespeare_data: \" + e.args[0])\n","\n","def get_imposter_by_name(name):\n","    global data_base_dict\n","    try:\n","      path = colab_path + \"data_base/imposters\"\n","      file_list = glob.glob(os.path.join(path, name, \"*.txt\"))\n","      data_list = create_data_list(file_list)\n","      data_base_dict[name] = data_list\n","    except Exception as e:\n","      print(\"Error at get_imposter_by_name: \" + e.args[0])\n","\n","def create_data_list(file_list):\n","    try:\n","        data = []\n","        for file_path in file_list:\n","            with open(file_path, \"r\", encoding='utf-8', errors='ignore') as f_input:\n","                data.append(f_input.read().replace('\\n', ' '))\n","        return data\n","    except Exception as e:\n","        print(\"Error at create_data_list: \" + e.args[0])\n","        return []\n","\n","def get_imposters_name_list():\n","    try:\n","        path = colab_path + \"data_base/imposters\"\n","        names = os.listdir(path)\n","        return names\n","    except Exception as e:\n","        print(\"Error at get_imposters_name_list: \" + str(e))\n","        return []\n","\n","def getBooksNameList(name):\n","    mypath = colab_path + \"data_base/\"\n","    if name == \"shakespeare\" or name == \"Shakespeare\":\n","        mypath = mypath + \"shakespeare\"\n","        onlyfiles = [f for f in os.listdir(mypath) if os.path.isfile(os.path.join(mypath, f))]\n","    else:\n","        mypath = mypath + \"imposters\"\n","        onlyfiles = [f for f in os.listdir(mypath) if os.path.isfile(os.path.join(mypath, f))]\n","    for file in onlyfiles:\n","        name, ext = os.path.splitext(file)\n","        if ext != \".txt\":\n","            onlyfiles.remove(file)\n","    return onlyfiles\n","\n","\n","def intersection(lst1, lst2):\n","    return list(set(lst1) & set(lst2))\n","\n","def calc_frac(a):\n","    \"\"\"a function that returns the fraction of ones in each column or row\"\"\"\n","    s = []\n","    for a0 in a:\n","     s.append(1-np.array(((a0 > a).sum())/len(a)))\n","    return(s)\n","\n","def presentation(dist_mat, pr, forest_num, files, k, pr0, str0, lines):\n","\n","    clf = IsolationForest(n_estimators=forest_num, warm_start=True)\n","\n","    clf.set_params(n_estimators=forest_num)  # add 10 more trees\n","    clf.fit(dist_mat)\n","    y_pred_train = clf.predict(dist_mat)\n","\n","    score = clf.decision_function(dist_mat)\n","    index0 = np.where(score < pr)\n","    labels = np.zeros(len(score))\n","    labels[index0] = 1\n","    print('\\n++++++++++++++++++++++++++++++++++++')\n","    print(str0, '\\n')\n","    fl = []\n","    for indd in index0[0]:\n","        print(files[indd], indd)\n","        fl.append(files[indd])\n","    print('\\n++++++++++++++++++++++++++++++++++++')\n","    rang = len(intersection(fl, lines))\n","\n","    #summa = [np.sum(np.sort(s)[1:k+1])/k for s in dist_mat]\n","    summa = [np.sum(s) for s in dist_mat]\n","    summa = summa/sum(summa)\n","    index0 = np.where(summa > np.percentile(summa, pr0))\n","    for indd in index0[0]:\n","        print(files[indd], indd)\n","    print('\\n++++++++++++++++++++++++++++++++++++')\n","   #return rankdata(summa), score, y_pred_train, rang\n","    return summa, score, y_pred_train, rang\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1I0_IoSNMKyIsnLCsZm8P-xlxErk8b3V_"},"id":"CzJuM4TrVccG","outputId":"ad8b7e24-eda8-4a4a-b93e-c305330bf8f7"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["if __name__ == \"__main__\":\n","    main()"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[{"file_id":"12hWiMi3PglPWOPLYjkcRlLi4tuvZhuDv","timestamp":1686244462960}],"authorship_tag":"ABX9TyPINgG5UbrSkjsOyd76Odcy"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}